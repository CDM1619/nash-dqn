diff --git a/launch.py b/launch.py
index cd1948c..a503298 100644
--- a/launch.py
+++ b/launch.py
@@ -1,9 +1,11 @@
 import supersuit, gym
 import torch
+from torch.utils.tensorboard import SummaryWriter
 import numpy as np
+from datetime import datetime
 from common.wrappers import reward_lambda_v1, zero_sum_reward_filer, SSVecWrapper
 from nash_dqn import NashDQN
-from common.args_parser import get_args
+from common.args_parser import get_args, init_wandb
 
 
 def rollout(env, model, args):
@@ -20,6 +22,15 @@ def rollout(env, model, args):
     ## Initialization
     print("Arguments: ", args)
     overall_steps = 0
+    if args.wandb_activate:
+        init_wandb(args)
+    time_string = datetime.now().strftime("%Y-%m-%d-%H-%M-%S")
+    run_name = '_'.join((args.env_name, args.algorithm, time_string))
+    writer = SummaryWriter(f"runs/{run_name}")
+    writer.add_text(
+        "hyperparameters",
+        "|param|value|\n|-|-|\n%s" % ("\n".join([f"|{key}|{value}|" for key, value in vars(args).items()])),
+    )
 
     def choose_action(states, args, model):
         greedy = True if args.test else False
@@ -105,8 +116,10 @@ def rollout(env, model, args):
             ):  # if any player in a game is done, the game episode done; may not be correct for some envs
                 break
         
-        print(epi, reward, loss)
-            
+        # print(epi, reward, loss)
+        writer.add_scalar(f"charts/episodic_return-player{0}", np.mean(reward, axis=0)[0], epi)
+        writer.add_scalar(f"charts/loss", loss, epi)
+
         # logger.log_episode_reward(step)
 
         ## Evaluation during exploiter training
